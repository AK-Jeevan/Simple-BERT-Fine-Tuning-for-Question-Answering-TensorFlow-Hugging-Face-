# ğŸ§  Simple BERT Fine-Tuning for Question Answering

A minimal TensorFlow implementation of **BERT fine-tuning** for the **SQuAD Question Answering** task â€” no sliding window, no extra preprocessing, just clean and simple code for educational and experimental use.

---
## ğŸ¤— datasets

**SQuAD v1.1 dataset** from Datasets Library.

## ğŸš€ Features

- âœ… Uses **Hugging Face Transformers** (`TFBertForQuestionAnswering`)
- âœ… Token alignment via offset mapping
- âœ… Trains and evaluates on the **SQuAD v1.1 dataset**
- âœ… Includes an easy-to-use **inference function**
- âœ… Compact, well-commented, and ideal for learning

---

## ğŸ§© Requirements

Install the dependencies:

pip install tensorflow transformers datasets

## ğŸ§± Project Structure
bert-qa-finetuning/
â”‚
â”œâ”€â”€ bert_qa_train.py       # Main training script
â”œâ”€â”€ README.md             
â””â”€â”€ requirements.txt       # dependencies list

## ğŸ“ˆ Notes

The implementation is non-sliding, meaning it may truncate long contexts.

For production or SQuAD benchmarks, use the Hugging Face Trainer API.

This script is ideal for educational purposes, demonstrations, or lightweight fine-tuning experiments.

## ğŸ’¡ Author

Created by: Krupa Jeevan
Inspired by: Hugging Face team & TensorFlow community
License: MIT License

â­ If you find this project helpful, give it a star on GitHub!

Feel free to modify the Repo and the CodeğŸ˜ŠğŸ˜ŠğŸ˜‹ğŸ˜‹
